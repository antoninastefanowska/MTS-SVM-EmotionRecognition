{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "federal-butler",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, multiprocessing, itertools, time, datetime, soundfile\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import essentia.standard as es\n",
    "import librosa as lib\n",
    "\n",
    "from random import randrange, sample, random\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit, StratifiedShuffleSplit, GridSearchCV, GroupKFold, KFold\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold, f_classif\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-expense",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "southeast-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, random_state, train_dataset_size, validation_dataset_size):\n",
    "    gss = GroupShuffleSplit(n_splits=10, train_size=train_dataset_size, random_state=random_state)\n",
    "    original_dataset = dataset[dataset['augmentation'] == 'no']\n",
    "    original_dataset = dataset\n",
    "    for train_idx, test_validation_idx in gss.split(original_dataset, original_dataset['emotion'], original_dataset['speaker']):\n",
    "        train_dataset = dataset.iloc[train_idx]\n",
    "        test_validation_dataset = dataset.iloc[test_validation_idx]\n",
    "    if validation_dataset_size == 0:\n",
    "        return train_dataset, train_dataset, test_validation_dataset\n",
    "        \n",
    "    gss = GroupShuffleSplit(n_splits=1, train_size=validation_dataset_size / (1 - train_dataset_size), random_state=random_state)\n",
    "    for validation_idx, test_idx in gss.split(test_validation_dataset, test_validation_dataset['emotion'], test_validation_dataset['speaker']):\n",
    "        test_dataset = test_validation_dataset.iloc[test_idx]\n",
    "        validation_dataset = test_validation_dataset.iloc[validation_idx]\n",
    "    \n",
    "    augmented_dataset = dataset[(dataset['augmentation'] != 'no') & (dataset['original_id'].isin(train_dataset['id']))]\n",
    "    train_dataset = pd.concat([train_dataset, augmented_dataset])\n",
    "    return train_dataset, validation_dataset, test_dataset\n",
    "\n",
    "def split_dataset_speaker_dependent(dataset, random_state, train_dataset_size, validation_dataset_size):\n",
    "    sss = StratifiedShuffleSplit(n_splits=10, train_size=train_dataset_size, random_state=random_state)\n",
    "    original_dataset = dataset[dataset['augmentation'] == 'no']\n",
    "    for train_idx, test_validation_idx in sss.split(original_dataset, original_dataset['emotion']):\n",
    "        train_dataset = dataset.iloc[train_idx]\n",
    "        test_validation_dataset = dataset.iloc[test_validation_idx]\n",
    "    if validation_dataset_size == 0:\n",
    "        return train_dataset, train_dataset, test_validation_dataset\n",
    "        \n",
    "    sss = StratifiedShuffleSplit(n_splits=1, train_size=validation_dataset_size / (1 - train_dataset_size), random_state=random_state)\n",
    "    for validation_idx, test_idx in sss.split(test_validation_dataset, test_validation_dataset['emotion']):\n",
    "        test_dataset = test_validation_dataset.iloc[test_idx]\n",
    "        validation_dataset = test_validation_dataset.iloc[validation_idx]\n",
    "        \n",
    "    augmented_dataset = dataset[(dataset['augmentation'] != 'no') & (dataset['original_id'].isin(train_dataset['id']))]\n",
    "    train_dataset = pd.concat([train_dataset, augmented_dataset])\n",
    "    return train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-gossip",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-amplifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue = pd.read_csv('./data_catalogue.csv', sep=';')\n",
    "original_data = catalogue[(catalogue['augmentation'] == 'no')]\n",
    "\n",
    "for index, row in original_data.iterrows():\n",
    "    name, extension = os.path.splitext(os.path.basename(row['filepath']))\n",
    "    audio, sample_rate = lib.load(row['filepath'])\n",
    "    directory_path = './datasets/augmentation/' + row['dataset'] + '/'\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "    \n",
    "    filepath1 = directory_path + name + '_pitch_shift_up' + extension\n",
    "    if filepath1 not in catalogue['filepath']:\n",
    "        modified1 = lib.effects.pitch_shift(audio, sample_rate, n_steps=-3)\n",
    "        soundfile.write(filepath1, modified1, sample_rate)\n",
    "        new_row1 = {\n",
    "            'id': catalogue.shape[0],\n",
    "            'filepath': filepath1,\n",
    "            'dataset': row['dataset'],\n",
    "            'speaker': row['speaker'],\n",
    "            'sex': row['sex'],\n",
    "            'age': row['age'],\n",
    "            'emotion': row['emotion'],\n",
    "            'augmentation': 'pitch_shift',\n",
    "            'original_id': row['id']\n",
    "        }\n",
    "        catalogue = catalogue.append(new_row1, ignore_index=True)\n",
    "    \n",
    "    filepath2 = directory_path + name + '_pitch_shift_down' + extension\n",
    "    if filepath2 not in catalogue['filepath']:\n",
    "        modified2 = lib.effects.pitch_shift(audio, sample_rate, n_steps=3)\n",
    "        soundfile.write(filepath2, modified2, sample_rate)\n",
    "        new_row2 = {\n",
    "            'id': catalogue.shape[0],\n",
    "            'filepath': filepath2,\n",
    "            'dataset': row['dataset'],\n",
    "            'speaker': row['speaker'],\n",
    "            'sex': row['sex'],\n",
    "            'age': row['age'],\n",
    "            'emotion': row['emotion'],\n",
    "            'augmentation': 'pitch_shift',\n",
    "            'original_id': row['id']\n",
    "        }\n",
    "        catalogue = catalogue.append(new_row2, ignore_index=True)\n",
    "        \n",
    "    filepath3 = directory_path + name + '_' + '_time_stretch_fast' + extension\n",
    "    if filepath3 not in catalogue['filepath']:\n",
    "        modified3 = lib.effects.time_stretch(audio, 1.25)\n",
    "        soundfile.write(filepath3, modified3, sample_rate)\n",
    "        new_row3 = {\n",
    "            'id': catalogue.shape[0],\n",
    "            'filepath': filepath3,\n",
    "            'dataset': row['dataset'],\n",
    "            'speaker': row['speaker'],\n",
    "            'sex': row['sex'],\n",
    "            'age': row['age'],\n",
    "            'emotion': row['emotion'],\n",
    "            'augmentation': 'time_stretch',\n",
    "            'original_id': row['id']\n",
    "        }\n",
    "        catalogue = catalogue.append(new_row3, ignore_index=True)\n",
    "        \n",
    "    filepath4 = directory_path + name + '_' + '_time_stretch_slow' + extension\n",
    "    if filepath4 not in catalogue['filepath']:\n",
    "        modified4 = lib.effects.time_stretch(audio, 0.75)\n",
    "        soundfile.write(filepath4, modified4, sample_rate)\n",
    "        new_row4 = {\n",
    "            'id': catalogue.shape[0],\n",
    "            'filepath': filepath4,\n",
    "            'dataset': row['dataset'],\n",
    "            'speaker': row['speaker'],\n",
    "            'sex': row['sex'],\n",
    "            'age': row['age'],\n",
    "            'emotion': row['emotion'],\n",
    "            'augmentation': 'time_stretch',\n",
    "            'original_id': row['id']\n",
    "        }\n",
    "        catalogue = catalogue.append(new_row4, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-princess",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "inclusive-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue = pd.read_csv('./data_catalogue.csv', sep=';')\n",
    "\n",
    "dataset = catalogue[catalogue['dataset'] == 'RAVDESS']\n",
    "dataset = dataset.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "handy-europe",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = dataset['dataset'].unique()\n",
    "original_dataset = dataset[(dataset['augmentation'] == 'no') & (dataset['emotion'].isin(['anger', 'sadness', 'happiness', 'fear', 'disgust']))]\n",
    "sizes = np.array([len(original_dataset[original_dataset['dataset'] == corpus]) for corpus in corpora])\n",
    "min_size = sizes.min()\n",
    "samples = []\n",
    "\n",
    "for corpus in corpora:\n",
    "    corpus_dataset = original_dataset[original_dataset['dataset'] == corpus]\n",
    "    if (len(corpus_dataset) > min_size):\n",
    "        n = int(min_size / len(corpus_dataset['emotion'].unique()) / len(corpus_dataset['sex'].unique()))\n",
    "        corpus_sample = corpus_dataset.groupby(['emotion', 'sex'], as_index=False, group_keys=False).apply(lambda data: data.sample(n=n, random_state=10))\n",
    "    else:\n",
    "        corpus_sample = corpus_dataset\n",
    "    \n",
    "    augmented_sample = dataset[(dataset['original_id'].isin(corpus_sample['id'])) & (dataset['augmentation'] != 'no')]\n",
    "    samples.append(pd.concat([corpus_sample, augmented_sample]))\n",
    "\n",
    "dataset = pd.concat(samples)\n",
    "dataset = dataset.reset_index()\n",
    "dataset = dataset.drop('level_0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "embedded-supervision",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset, test_dataset = split_dataset_speaker_dependent(dataset, 8, 0.7, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-tourism",
   "metadata": {},
   "source": [
    "# FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "impressed-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(audio):\n",
    "    audio -= audio.mean()\n",
    "    audio /= audio.std()\n",
    "    return audio\n",
    "\n",
    "def trim(audio, percentage):\n",
    "    absolute = np.abs(audio)\n",
    "    threshold = (absolute.max() - absolute.min()) * percentage / 100\n",
    "    for start, sample in enumerate(absolute):\n",
    "        if sample > threshold:\n",
    "            break\n",
    "    for end, sample in reversed(list(enumerate(absolute))):\n",
    "        if sample > threshold:\n",
    "            break\n",
    "    audio = audio[start:end]\n",
    "    return audio\n",
    "\n",
    "def preprocess(audio, low_pass, high_pass):\n",
    "    lowpass = es.LowPass(cutoffFrequency=low_pass)\n",
    "    audio = lowpass(audio)\n",
    "    if high_pass != 0:\n",
    "        highpass = es.HighPass(cutoffFrequency=high_pass)\n",
    "        audio = highpass(audio)\n",
    "    audio = trim(audio, 5)\n",
    "    audio = normalize(audio)\n",
    "    return audio\n",
    "\n",
    "def essentia_mfcc(filepath, options):\n",
    "    window = es.Windowing(type=options['windowing'], size=options['frame_size'])\n",
    "    spectrum = es.Spectrum(size=options['frame_size'])\n",
    "    mfcc = es.MFCC(\n",
    "        type=options['type'],\n",
    "        warpingFormula=options['warping_formula'],\n",
    "        weighting=options['weighting'],\n",
    "        highFrequencyBound=options['high_frequency_bound'],\n",
    "        numberBands=options['number_bands'],\n",
    "        numberCoefficients=options['number_coefficients'],\n",
    "        normalize=options['normalize'],\n",
    "        dctType=options['dct_type'],\n",
    "        logType=options['log_type'],\n",
    "        liftering=options['liftering'],\n",
    "        lowFrequencyBound=options['low_frequency_bound'],\n",
    "        inputSize=(int(options['frame_size'] / 2) + 1))\n",
    "    \n",
    "    loader = es.MonoLoader(filename=filepath)\n",
    "    audio = loader()\n",
    "    audio = preprocess(audio, options['low_pass'], options['high_pass'])\n",
    "    mfccs = []\n",
    "    for frame in es.FrameGenerator(audio,\n",
    "            frameSize=options['frame_size'],\n",
    "            hopSize=int(options['frame_size'] / 4),\n",
    "            startFromZero=True,\n",
    "            validFrameThresholdRatio=1):\n",
    "        spect = spectrum(window(frame))\n",
    "        _, mfcc_coeffs = mfcc(spect)\n",
    "        mfccs.append(mfcc_coeffs)\n",
    "    mfccs = np.array(mfccs).T\n",
    "    coeffs = {\n",
    "        'mfcc': mfccs.shape[0]\n",
    "    }\n",
    "    return mfccs, coeffs\n",
    "\n",
    "def essentia_pitch(filepath, options):\n",
    "    window = es.Windowing(type='hann', size=1102)\n",
    "    spectrum = es.Spectrum(size=1102)\n",
    "    pitch = es.PitchYinFFT(\n",
    "        frameSize=1102,\n",
    "        interpolate=options['interpolate'],\n",
    "        maxFrequency=options['max_frequency'],\n",
    "        minFrequency=options['min_frequency'],\n",
    "        tolerance=options['tolerance'])\n",
    "    \n",
    "    loader = es.MonoLoader(filename=filepath)\n",
    "    audio = loader()\n",
    "    audio = preprocess(audio, 1000, 0)\n",
    "    pitches = []\n",
    "    for frame in es.FrameGenerator(audio,\n",
    "            frameSize=1102,\n",
    "            hopSize=int(1102 / 4),\n",
    "            startFromZero=True,\n",
    "            validFrameThresholdRatio=1):\n",
    "        spect = spectrum(window(frame))\n",
    "        pitch_value, _ = pitch(spect)\n",
    "        pitches.append(pitch_value)\n",
    "    pitches = np.expand_dims(np.array(pitches), axis=0)\n",
    "    coeffs = {\n",
    "        'pitch': pitches.shape[0]\n",
    "    }\n",
    "    return pitches, coeffs\n",
    "\n",
    "def essentia_features(filepath, options):\n",
    "    window = es.Windowing(type=options['windowing'], size=options['frame_size'])\n",
    "    spectrum = es.Spectrum(size=options['frame_size'])\n",
    "    mfcc = es.MFCC(\n",
    "        type=options['type'],\n",
    "        warpingFormula=options['warping_formula'],\n",
    "        weighting=options['weighting'],\n",
    "        highFrequencyBound=options['high_frequency_bound'],\n",
    "        numberBands=options['number_bands'],\n",
    "        numberCoefficients=options['number_coefficients'],\n",
    "        normalize=options['normalize'],\n",
    "        dctType=options['dct_type'],\n",
    "        logType=options['log_type'],\n",
    "        liftering=options['liftering'],\n",
    "        lowFrequencyBound=options['low_frequency_bound'],\n",
    "        inputSize=(int(options['frame_size'] / 2) + 1))\n",
    "    zcr = es.ZeroCrossingRate()\n",
    "    pitch = es.PitchYinFFT(frameSize=options['frame_size'])\n",
    "    flux = es.Flux()\n",
    "    \n",
    "    loader = es.MonoLoader(filename=filepath)\n",
    "    audio = loader()\n",
    "    audio = preprocess(audio, options['low_pass'], options['high_pass'])\n",
    "    mfccs = []\n",
    "    zcrs = []\n",
    "    zcrs_delta = []\n",
    "    zcrs_delta_delta = []\n",
    "    pitches = []\n",
    "    pitches_delta = []\n",
    "    pitches_delta_delta = []\n",
    "    fluxes = []\n",
    "    fluxes_delta = []\n",
    "    fluxes_delta_delta = []\n",
    "    for frame in es.FrameGenerator(audio,\n",
    "            frameSize=options['frame_size'],\n",
    "            hopSize=int(options['frame_size'] / 4),\n",
    "            startFromZero=True,\n",
    "            validFrameThresholdRatio=1):\n",
    "        spect = spectrum(window(frame))\n",
    "        _, mfcc_coeffs = mfcc(spect)\n",
    "        \n",
    "        pitch_value, _ = pitch(spect)        \n",
    "        pitch_delta = pitch_value - pitches[len(pitches) - 1] if len(pitches) > 0 else 0\n",
    "        pitch_delta_delta = pitch_delta - pitches_delta[len(pitches_delta) - 1] if len(pitches_delta) > 0 else 0\n",
    "        \n",
    "        flux_value = flux(spect)\n",
    "        flux_delta = flux_value - fluxes[len(fluxes) - 1] if len(fluxes) > 0 else 0\n",
    "        flux_delta_delta = flux_delta - fluxes_delta[len(fluxes_delta) - 1] if len(fluxes_delta) > 0 else 0\n",
    "        \n",
    "        zcr_value = zcr(frame)\n",
    "        zcr_delta = zcr_value - zcrs[len(zcrs) - 1] if len(zcrs) > 0 else 0\n",
    "        zcr_delta_delta = zcr_delta - zcrs_delta[len(zcrs_delta) - 1] if len(zcrs_delta) > 0 else 0\n",
    "        \n",
    "        mfccs.append(mfcc_coeffs)\n",
    "        pitches.append(pitch_value)\n",
    "        pitches_delta.append(pitch_delta)\n",
    "        pitches_delta_delta.append(pitch_delta_delta)\n",
    "        \n",
    "        fluxes.append(flux_value)\n",
    "        fluxes_delta.append(flux_delta)\n",
    "        fluxes_delta_delta.append(flux_delta_delta)\n",
    "        \n",
    "        zcrs.append(zcr_value)\n",
    "        zcrs_delta.append(zcr_delta)\n",
    "        zcrs_delta_delta.append(zcr_delta_delta)\n",
    "        \n",
    "    mfccs = np.array(mfccs).T\n",
    "    zcrs = np.expand_dims(np.array(zcrs), axis=0)\n",
    "    zcrs_delta = np.expand_dims(np.array(zcrs_delta), axis=0)\n",
    "    zcrs_delta_delta = np.expand_dims(np.array(zcrs_delta_delta), axis=0)\n",
    "    \n",
    "    pitches = np.expand_dims(np.array(pitches), axis=0)\n",
    "    pitches_delta = np.expand_dims(np.array(pitches_delta), axis=0)\n",
    "    pitches_delta_delta = np.expand_dims(np.array(pitches_delta_delta), axis=0)\n",
    "    \n",
    "    fluxes = np.expand_dims(np.array(fluxes), axis=0)\n",
    "    fluxes_delta = np.expand_dims(np.array(fluxes_delta), axis=0)\n",
    "    fluxes_delta_delta = np.expand_dims(np.array(fluxes_delta_delta), axis=0)\n",
    "\n",
    "    coeffs = {\n",
    "        'mfcc': mfccs.shape[0],\n",
    "        'flux': fluxes.shape[0],\n",
    "        'flux_delta': fluxes_delta.shape[0],\n",
    "        'flux_delta_delta': fluxes_delta_delta.shape[0],\n",
    "        'pitch': pitches.shape[0],\n",
    "        'pitch_delta': pitches_delta.shape[0],\n",
    "        'pitch_delta_delta': pitches_delta_delta.shape[0],\n",
    "        'zcr': zcrs.shape[0],\n",
    "        'zcr_delta': zcrs_delta.shape[0],\n",
    "        'zcr_delta_delta': zcrs_delta_delta.shape[0]\n",
    "    }\n",
    "    return np.concatenate((mfccs, fluxes, fluxes_delta, fluxes_delta_delta, pitches, pitches_delta, pitches_delta_delta, zcrs, zcrs_delta, zcrs_delta_delta)), coeffs\n",
    "\n",
    "def get_raw_features_single(dataset, options, index):\n",
    "    row = dataset.iloc[index]\n",
    "    features, _ = essentia_features(row['filepath'], options)\n",
    "    return features\n",
    "\n",
    "def get_raw_features(dataset, options):\n",
    "    raw_features = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        features, _ = essentia_features(row['filepath'], options)\n",
    "        raw_features.append(features)\n",
    "    return raw_features\n",
    "\n",
    "def get_feature_blocks(raw_features, block_number, overlap):\n",
    "    all_blocks = []\n",
    "    for feature in raw_features:\n",
    "        block_size = math.floor((feature.shape[1] - overlap) / block_number + overlap)\n",
    "        blocks = [np.asarray(feature[:, i : i + block_size]) for i in range(0, feature.shape[1], block_size - overlap)]\n",
    "        while len(blocks) > block_number:\n",
    "            blocks[len(blocks) - 2] = np.concatenate((blocks[len(blocks) - 2], blocks[len(blocks) - 1]), axis=1)\n",
    "            blocks.pop()\n",
    "\n",
    "        blocks_features = []\n",
    "        for block in blocks:\n",
    "            avg_stat = []\n",
    "            std_stat = []\n",
    "            median_stat = []\n",
    "            q1_stat = []\n",
    "            q2_stat = []\n",
    "            min_stat = []\n",
    "            max_stat = []\n",
    "            for feature in block:\n",
    "                avg_stat.append(feature.mean())\n",
    "                std_stat.append(feature.std())\n",
    "                median_stat.append(np.median(feature))\n",
    "                q1_stat.append(np.percentile(feature, 25))\n",
    "                q2_stat.append(np.percentile(feature, 75))\n",
    "                min_stat.append(feature.min())\n",
    "                max_stat.append(feature.max())\n",
    "            features = np.concatenate([avg_stat, std_stat, median_stat, q1_stat, q2_stat, min_stat, max_stat])\n",
    "            blocks_features.append(features)    \n",
    "        all_blocks.append(blocks_features)\n",
    "    \n",
    "    return np.asarray(all_blocks).transpose(1, 0, 2)\n",
    "\n",
    "def export_row_features(index):\n",
    "    row = catalogue.iloc[index]\n",
    "    features, _ = essentia_features(row['filepath'], options)\n",
    "    raw_features = [features]\n",
    "\n",
    "    small_block_features = get_feature_blocks(raw_features, 10, 2)\n",
    "    big_block_features = get_feature_blocks(raw_features, 3, 4)\n",
    "    single_block_features = get_feature_blocks(raw_features, 1, 0)\n",
    "\n",
    "    final_features = np.concatenate([small_block_features, big_block_features, single_block_features])\n",
    "\n",
    "    name, extension = os.path.splitext(os.path.basename(row['filepath']))\n",
    "    directory_path = './features/' + row['dataset'] + '/'\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "\n",
    "    filepath = directory_path + name + '_features.npy'\n",
    "    np.save(filepath, final_features)\n",
    "\n",
    "def export_features():\n",
    "    catalogue['feature_file'] = None\n",
    "    pool = multiprocessing.Pool()\n",
    "    pool.map(export_row_features, range(len(dataset)))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    for index, row in catalogue.iterrows():\n",
    "        name, extension = os.path.splitext(os.path.basename(row['filepath']))\n",
    "        directory_path = './features/' + row['dataset'] + '/'\n",
    "        filepath = directory_path + name + '_features.npy'\n",
    "        catalogue.at[index, 'feature_file'] = filepath\n",
    "        \n",
    "def import_dataset_features(dataset):\n",
    "    dataset_features = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        dataset_features.append(np.load(row['feature_file']))\n",
    "    dataset_features = np.concatenate(dataset_features, axis=1)\n",
    "    \n",
    "    small_block_features = dataset_features[0:10]\n",
    "    big_block_features = dataset_features[10:13]\n",
    "    single_block_features = np.expand_dims(dataset_features[13], axis=0)\n",
    "    \n",
    "    return small_block_features, big_block_features, single_block_features\n",
    "\n",
    "def get_feature_names(mfcc_number):\n",
    "    base_names = ['flux', 'flux_delta', 'flux_delta_delta', 'pitch', 'pitch_delta', 'pitch_delta_delta', 'zcr', 'zcr_delta', 'zcr_delta_delta']\n",
    "    stat_names = ['avg', 'std', 'median', 'q1', 'q2', 'min', 'max']\n",
    "    mfcc_names = ['mfcc_' + str(i) for i in range(0, mfcc_number)]\n",
    "    base_names = mfcc_names + base_names\n",
    "    \n",
    "    feature_names = []\n",
    "    for base_name in base_names:\n",
    "        for stat_name in stat_names:\n",
    "            feature_names.append(stat_name + '_' + base_name)\n",
    "    return feature_names\n",
    "\n",
    "def options_from_row(row):\n",
    "    options = {}\n",
    "    columns = row.index\n",
    "    columns = np.delete(columns, [len(columns) - 3, len(columns) - 2, len(columns) - 1])\n",
    "    for column in columns:\n",
    "        if type(row[column]).__module__ == 'numpy': \n",
    "            options[column] = row[column].item()\n",
    "        else:\n",
    "            options[column] = row[column]\n",
    "    return options\n",
    "\n",
    "def split_features(train_dataset, validation_dataset, test_dataset, small_block_features, big_block_features, single_block_features):\n",
    "    train_small_block_features = small_block_features[..., train_dataset.index, :]\n",
    "    validation_small_block_features = small_block_features[..., validation_dataset.index, :]\n",
    "    test_small_block_features = small_block_features[..., test_dataset.index, :]\n",
    "\n",
    "    train_big_block_features = big_block_features[..., train_dataset.index, :]\n",
    "    validation_big_block_features = big_block_features[..., validation_dataset.index, :]\n",
    "    test_big_block_features = big_block_features[..., test_dataset.index, :]\n",
    "\n",
    "    train_single_block_features = single_block_features[..., train_dataset.index, :]\n",
    "    validation_single_block_features = single_block_features[..., validation_dataset.index, :]\n",
    "    test_single_block_features = single_block_features[..., test_dataset.index, :]\n",
    "    \n",
    "    train_input = np.concatenate((train_small_block_features, train_big_block_features, train_single_block_features))\n",
    "    validation_input = np.concatenate((validation_small_block_features, validation_big_block_features, validation_single_block_features))\n",
    "    test_input = np.concatenate((test_small_block_features, test_big_block_features, test_single_block_features))\n",
    "    \n",
    "    return train_input, validation_input, test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "extended-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Specimen:\n",
    "    def __init__(self, classifier, train_dataset, test_dataset, options_set):\n",
    "        self.classifier = classifier\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.options_set = options_set\n",
    "        self.option_names = list(options_set.keys())\n",
    "        self.chromosome = []\n",
    "        \n",
    "        self.accuracy = None\n",
    "        \n",
    "    def init_random(self):\n",
    "        for option_index, option_name in enumerate(self.option_names):\n",
    "            self.add_random_gene()\n",
    "            \n",
    "    def init_ready(self, options, accuracy):\n",
    "        self.accuracy = accuracy\n",
    "        self.chromosome = []\n",
    "        for option in options:\n",
    "            self.chromosome.append(self.options_set[option].index(options[option]))\n",
    "    \n",
    "    def get_options(self):\n",
    "        options = dict.fromkeys(self.options_set.keys(), None)\n",
    "        for option_index, option_name in enumerate(self.option_names):\n",
    "            options[option_name] = self.options_set[option_name][self.chromosome[option_index]]\n",
    "        return options\n",
    "    \n",
    "    def calculate_fitness(self):\n",
    "        options = self.get_options()\n",
    "        try:\n",
    "            train_raw_features = get_raw_features(self.train_dataset, options)\n",
    "            test_raw_features = get_raw_features(self.test_dataset, options)\n",
    "            train_features = get_feature_blocks(train_raw_features, 1, 0)[0]\n",
    "            test_features = get_feature_blocks(test_raw_features, 1, 0)[0]\n",
    "        except RuntimeError:\n",
    "            self.accuracy = -1\n",
    "            return\n",
    "        \n",
    "        self.classifier.fit(train_features, self.train_dataset['emotion'])\n",
    "        predicted = self.classifier.predict(test_features)\n",
    "        self.accuracy = accuracy_score(predicted, self.test_dataset['emotion'])\n",
    "    \n",
    "    def get_fitness(self):\n",
    "        return self.accuracy\n",
    "        \n",
    "    def add_gene(self, gene):\n",
    "        self.chromosome.append(gene)\n",
    "        \n",
    "    def add_random_gene(self):\n",
    "        option_name = self.option_names[len(self.chromosome)]\n",
    "        values_number = len(self.options_set[option_name])\n",
    "        random_gene = randrange(values_number)\n",
    "        self.add_gene(random_gene)\n",
    "        \n",
    "    def get_gene(self, index):\n",
    "        return self.chromosome[index]\n",
    "    \n",
    "    def equals(self, other_specimen):\n",
    "        for index, gene in enumerate(self.chromosome):\n",
    "            if gene != other_specimen.chromosome[index]:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def to_string(self):\n",
    "        return ''.join(str(x) for x in self.chromosome)\n",
    "\n",
    "class GeneticAlgorithm:\n",
    "    def __init__(self, classifier, population_size, crossover_possibility, mutation_possibility, tournament_size, history, train_dataset, test_dataset, options_set):\n",
    "        self.classifier = classifier\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.options_set = options_set\n",
    "        self.option_names = list(options_set.keys())\n",
    "        \n",
    "        self.population_size = population_size\n",
    "        self.crossover_possibility = crossover_possibility\n",
    "        self.mutation_possibility = mutation_possibility\n",
    "        self.tournament_size = tournament_size\n",
    "        self.current_iteration = 0\n",
    "        self.population = []\n",
    "        self.history = history\n",
    "        \n",
    "        self.init_population()\n",
    "        \n",
    "    def init_population(self):\n",
    "        for i in range(0, self.population_size):\n",
    "            specimen = self.create_specimen()\n",
    "            specimen.init_random()\n",
    "            specimen = self.find_in_history(specimen)\n",
    "            if not self.exists(self.population, specimen) and specimen.accuracy != -1:\n",
    "                self.population.append(specimen)\n",
    "            else:\n",
    "                i -= 1\n",
    "                \n",
    "        self.calculate_population_fitness(self.population, 8)\n",
    "                \n",
    "    def calculate_chunk_fitness(self, chunk, output):\n",
    "        for index, specimen in enumerate(chunk):\n",
    "            specimen.calculate_fitness()\n",
    "            output[specimen.to_string()] = specimen.accuracy\n",
    "                \n",
    "    def calculate_population_fitness(self, population, cores):\n",
    "        unprepared = np.asarray([specimen for specimen in population if specimen.get_fitness() is None])\n",
    "        manager = multiprocessing.Manager()\n",
    "        output = manager.dict()\n",
    "        chunks = np.array_split(unprepared, cores)\n",
    "        jobs = []\n",
    "        for index, chunk in enumerate(chunks):\n",
    "            p = multiprocessing.Process(target=self.calculate_chunk_fitness, args=(chunk, output))\n",
    "            jobs.append(p)\n",
    "            p.start()\n",
    "\n",
    "        for proc in jobs:\n",
    "            proc.join()\n",
    "        \n",
    "        for specimen in unprepared:\n",
    "            specimen.accuracy = output[specimen.to_string()]\n",
    "        \n",
    "    def iteration(self):\n",
    "        self.current_iteration += 1\n",
    "        new_population = []\n",
    "        parents = self.tournament_selection(5)\n",
    "        pairs = list(itertools.combinations(parents, 2))\n",
    "        for pair in pairs:\n",
    "            if random() <= self.crossover_possibility: \n",
    "                offspring1, offspring2 = self.crossover(pair[0], pair[1])\n",
    "                offspring1 = self.find_in_history(offspring1)\n",
    "                offspring2 = self.find_in_history(offspring2)\n",
    "                if not self.exists(new_population, offspring1):\n",
    "                    new_population.append(offspring1)\n",
    "                if not self.exists(new_population, offspring2):\n",
    "                    new_population.append(offspring2)\n",
    "                \n",
    "        for parent in parents:\n",
    "            if random() <= self.mutation_possibility:\n",
    "                mutated = self.mutation(parent)\n",
    "                mutated = self.find_in_history(mutated)\n",
    "                if not self.exists(new_population, mutated):\n",
    "                    new_population.append(mutated)\n",
    "        \n",
    "        for i in range(0, 3):\n",
    "            if not self.exists(new_population, parents[i]):\n",
    "                new_population.append(parents[i])\n",
    "            \n",
    "        self.calculate_population_fitness(new_population, 8)\n",
    "        new_population.sort(key=lambda specimen: specimen.get_fitness(), reverse=True)\n",
    "        self.population = new_population[:self.population_size]\n",
    "    \n",
    "    def best(self):\n",
    "        return max(self.population, key=lambda specimen: specimen.get_fitness())\n",
    "    \n",
    "    def create_specimen(self):\n",
    "        specimen = Specimen(self.classifier, self.train_dataset, self.test_dataset, self.options_set)\n",
    "        return specimen\n",
    "        \n",
    "    def crossover(self, parent1, parent2):\n",
    "        offspring1 = self.create_specimen()\n",
    "        offspring2 = self.create_specimen()\n",
    "        for option_index, option_name in enumerate(self.option_names):\n",
    "            if random() < 0.5:\n",
    "                offspring1.add_gene(parent1.get_gene(option_index))\n",
    "                offspring2.add_gene(parent2.get_gene(option_index))\n",
    "            else:\n",
    "                offspring1.add_gene(parent2.get_gene(option_index))\n",
    "                offspring2.add_gene(parent1.get_gene(option_index))\n",
    "        return offspring1, offspring2\n",
    "    \n",
    "    def mutation(self, specimen):\n",
    "        mutated = self.create_specimen()\n",
    "        for option_index, option_name in enumerate(self.option_names):\n",
    "            if random() < 0.5:\n",
    "                mutated.add_random_gene()\n",
    "            else:\n",
    "                mutated.add_gene(specimen.get_gene(option_index))\n",
    "        return mutated\n",
    "        \n",
    "    def tournament_selection(self, size):\n",
    "        parents = []\n",
    "        old_population = [specimen for specimen in self.population]\n",
    "        while len(old_population) > self.tournament_size and len(parents) < size:\n",
    "            tournament = sample(old_population, self.tournament_size)\n",
    "            winner = max(tournament, key=lambda specimen: specimen.get_fitness())\n",
    "            parents.append(winner)\n",
    "            old_population.remove(winner)\n",
    "        if len(parents) < size:\n",
    "            parents[len(parents):size] = old_population[:size-len(parents)]\n",
    "        parents.sort(key=lambda specimen: specimen.get_fitness(), reverse=True)\n",
    "        return parents\n",
    "        \n",
    "    def find_in_history(self, specimen):\n",
    "        key = specimen.to_string()\n",
    "        if key in self.history:\n",
    "            return self.history[key]\n",
    "        else:\n",
    "            self.history[key] = specimen\n",
    "            return specimen\n",
    "    \n",
    "    def exists(self, population, specimen):\n",
    "        for existing_specimen in population:\n",
    "            if specimen.equals(existing_specimen):\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lucky-aircraft",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_options_set = {\n",
    "    'frame_size': [512, 756, 1024, 1102],\n",
    "    'windowing': ['hamming', 'hann'],\n",
    "    'warping_formula': ['slaneyMel', 'htkMel'],\n",
    "    'log_type': ['natural', 'dbpow', 'dbamp', 'log'],\n",
    "    'dct_type': [2, 3],\n",
    "    'normalize': ['unit_sum', 'unit_tri', 'unit_max'],\n",
    "    'high_frequency_bound': [6000, 8000, 16000, 20000],\n",
    "    'number_bands': [26, 128],\n",
    "    'number_coefficients': [10, 13, 20, 40, 80, 120],\n",
    "    'weighting': ['warping', 'linear'],\n",
    "    'type': ['magnitude', 'power'],\n",
    "    'liftering': [0, 22, 10, 40, 100],\n",
    "    'low_frequency_bound': [0, 50, 100, 200, 500],\n",
    "    'low_pass': [0, 1000, 2000, 5000, 7000],\n",
    "    'high_pass': [0, 8000, 15000, 20000, 100000]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-missouri",
   "metadata": {},
   "source": [
    "## Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-scotland",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.system(\"taskset -p 0xff %d\" % os.getpid())\n",
    "history = {}\n",
    "total_time = 0\n",
    "accuracies = []\n",
    "classifier = SVC()\n",
    "\n",
    "genetic_algorithm = GeneticAlgorithm(classifier, 10, 0.7, 0.5, 5, history, train_dataset, validation_dataset, mfcc_options_set)\n",
    "start_time = time.time()\n",
    "for i in range(0, 50):\n",
    "    genetic_algorithm.iteration()\n",
    "    best = genetic_algorithm.best()\n",
    "    \n",
    "    print()\n",
    "    print(best.get_options())\n",
    "    print(best.accuracy)\n",
    "    accuracies.append(best.accuracy)\n",
    "total_time += time.time() - start_time\n",
    "print('Total time:', datetime.timedelta(seconds=(total_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = list(mfcc_options_set.keys()) + ['accuracy'])\n",
    "\n",
    "for key in genetic_algorithm.history:\n",
    "    row = {}\n",
    "    specimen = genetic_algorithm.history[key]\n",
    "    for option in specimen.get_options():\n",
    "        row[option] = specimen.get_options()[option]\n",
    "    row['accuracy'] = specimen.accuracy\n",
    "    results = results.append(row, ignore_index=True)\n",
    "    \n",
    "results = results.drop(results[results['accuracy'].isnull()].index)\n",
    "row = results.sort_values('accuracy', ascending=False).iloc[0]\n",
    "options = options_from_row(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-brave",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "raw_features = Parallel(n_jobs=8)(delayed(get_raw_features_single)(dataset, options, index) for index in dataset.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "small_block_features = get_feature_blocks(raw_features, 10, 2)\n",
    "big_block_features = get_feature_blocks(raw_features, 3, 4)\n",
    "single_block_features = get_feature_blocks(raw_features, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-logistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, validation_input, test_input = split_features(train_dataset, validation_dataset, test_dataset, small_block_features, big_block_features, single_block_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-malta",
   "metadata": {},
   "source": [
    "## CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "framed-artist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_classifier_svm(dataset, train_dataset, validation_dataset, features):\n",
    "    k_best = SelectKBest(f_classif)\n",
    "    classifier = SVC(random_state=8)\n",
    "    pipeline = Pipeline(steps=[('filter_constants', VarianceThreshold(threshold=0)), ('k_best', k_best), ('scaler', StandardScaler()), ('svm', classifier)])\n",
    "    features_number = features.shape[1]\n",
    "\n",
    "    k = ['all', int(0.9 * features_number), int(0.5 * features_number), int(0.25 * features_number)]\n",
    "    C_values = [0.1, 1, 10]\n",
    "    kernel = ['linear', 'poly', 'rbf']\n",
    "    gamma_values = [1 / features_number] * np.array([0.1, 1, 10])\n",
    "    grid_search_parameters = {\n",
    "        'k_best__k': k,\n",
    "        'svm__C': C_values,\n",
    "        'svm__kernel': kernel,\n",
    "        'svm__gamma': gamma_values\n",
    "    }\n",
    "    grid_search = GridSearchCV(pipeline,\n",
    "                               grid_search_parameters,\n",
    "                               cv=[(train_dataset.index, validation_dataset.index)],\n",
    "                               n_jobs=8,\n",
    "                               scoring='accuracy',\n",
    "                               return_train_score=True)\n",
    "\n",
    "    grid_search.fit(features, dataset['emotion'])\n",
    "    return grid_search\n",
    "\n",
    "def cross_validate(dataset, single_block_features, folds, validation_dataset_size, random_state):\n",
    "    group_kfold = GroupKFold(n_splits=folds)\n",
    "    gss = GroupShuffleSplit(n_splits=1, train_size=1 - validation_dataset_size / (1 - 1 / folds), random_state=random_state)\n",
    "    accuracies_validation = []\n",
    "    accuracies_test = []\n",
    "    original_dataset = dataset[dataset['augmentation'] == 'no']\n",
    "    for train_validation_idx, test_idx in group_kfold.split(original_dataset, original_dataset['emotion'], original_dataset['speaker']):\n",
    "        train_validation_dataset = dataset.iloc[train_validation_idx]\n",
    "        test_dataset = dataset.iloc[test_idx]\n",
    "        for train_idx, validation_idx in gss.split(train_validation_dataset, train_validation_dataset['emotion'], train_validation_dataset['speaker']):\n",
    "            train_dataset = train_validation_dataset.iloc[train_idx]\n",
    "            validation_dataset = train_validation_dataset.iloc[validation_idx]\n",
    "        \n",
    "        tune_train_dataset = train_dataset\n",
    "        augmented_dataset = dataset[(dataset['augmentation'] != 'no') & (dataset['original_id'].isin(train_dataset['id']))]\n",
    "        train_dataset = pd.concat([train_dataset, augmented_dataset])\n",
    "        \n",
    "        train_features = single_block_features[0, train_dataset.index, :]\n",
    "        test_features = single_block_features[0, test_dataset.index, :]\n",
    "\n",
    "        grid_search = tune_classifier_svm(dataset, tune_train_dataset, validation_dataset, single_block_features[0])\n",
    "        classifier = grid_search.best_estimator_\n",
    "        classifier.fit(train_features, train_dataset['emotion'])\n",
    "\n",
    "        validation_accuracy = grid_search.best_score_\n",
    "        accuracies_validation.append(validation_accuracy)\n",
    "\n",
    "        predicted = classifier.predict(test_features)\n",
    "        test_accuracy = accuracy_score(predicted, test_dataset['emotion'])\n",
    "        accuracies_test.append(test_accuracy)\n",
    "        print(validation_accuracy, test_accuracy)\n",
    "    return accuracies_validation, accuracies_test\n",
    "\n",
    "def cross_validate_memory_efficient(dataset, folds, validation_dataset_size, random_state):\n",
    "    group_kfold = GroupKFold(n_splits=folds)\n",
    "    gss = GroupShuffleSplit(n_splits=1, train_size=1 - validation_dataset_size / (1 - 1 / folds), random_state=random_state)\n",
    "    accuracies_validation = []\n",
    "    accuracies_test = []\n",
    "    original_dataset = dataset[dataset['augmentation'] == 'no']\n",
    "    for train_validation_idx, test_idx in group_kfold.split(original_dataset, original_dataset['emotion'], original_dataset['speaker']):\n",
    "        train_validation_dataset = dataset.iloc[train_validation_idx]\n",
    "        test_dataset = dataset.iloc[test_idx]\n",
    "        for train_idx, validation_idx in gss.split(train_validation_dataset, train_validation_dataset['emotion'], train_validation_dataset['speaker']):\n",
    "            train_dataset = train_validation_dataset.iloc[train_idx]\n",
    "            validation_dataset = train_validation_dataset.iloc[validation_idx]\n",
    "        \n",
    "        tune_train_dataset = train_dataset\n",
    "        n = int(1000 / len(tune_train_dataset['emotion'].unique()) / len(tune_train_dataset['sex'].unique()))\n",
    "        tune_train_dataset = tune_train_dataset.groupby(['emotion', 'sex'], as_index=False, group_keys=False).apply(lambda data: data.sample(n=n, random_state=10))\n",
    "        tune_train_dataset = tune_train_dataset.reset_index()\n",
    "        validation_dataset.index = range(len(tune_train_dataset), len(tune_train_dataset) + len(validation_dataset))\n",
    "        tune_dataset = pd.concat([tune_train_dataset, validation_dataset])\n",
    "        \n",
    "        augmented_dataset = dataset[(dataset['augmentation'] != 'no') & (dataset['original_id'].isin(train_dataset['id']))]\n",
    "        train_dataset = pd.concat([train_dataset, augmented_dataset])\n",
    "        \n",
    "        small_block_features, big_block_features, single_block_features = import_dataset_features(tune_dataset)\n",
    "        grid_search = tune_classifier_svm(tune_dataset, tune_train_dataset, validation_dataset, single_block_features[0])\n",
    "        \n",
    "        small_block_features, big_block_features, single_block_features = import_dataset_features(train_dataset)\n",
    "        train_features = single_block_features[0]\n",
    "        \n",
    "        small_block_features, big_block_features, single_block_features = import_dataset_features(test_dataset)\n",
    "        test_features = single_block_features[0]\n",
    "                \n",
    "        del small_block_features\n",
    "        del big_block_features\n",
    "        del single_block_features\n",
    "        \n",
    "        classifier = grid_search.best_estimator_\n",
    "        classifier.fit(train_features, train_dataset['emotion'])\n",
    "\n",
    "        validation_accuracy = grid_search.best_score_\n",
    "        accuracies_validation.append(validation_accuracy)\n",
    "\n",
    "        predicted = classifier.predict(test_features)\n",
    "        test_accuracy = accuracy_score(predicted, test_dataset['emotion'])\n",
    "        accuracies_test.append(test_accuracy)\n",
    "        print(validation_accuracy, test_accuracy)\n",
    "    return accuracies_validation, accuracies_test\n",
    "\n",
    "def cross_validate_speaker_dependent(dataset, single_block_features, folds, validation_dataset_size, random_state):\n",
    "    kfold = KFold(n_splits=folds)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, train_size=1 - validation_dataset_size / (1 - 1 / folds), random_state=random_state)\n",
    "    accuracies_validation = []\n",
    "    accuracies_test = []\n",
    "    original_dataset = dataset[dataset['augmentation'] == 'no']\n",
    "    for train_validation_idx, test_idx in kfold.split(original_dataset, original_dataset['emotion']):\n",
    "        train_validation_dataset = dataset.iloc[train_validation_idx]\n",
    "        test_dataset = dataset.iloc[test_idx]\n",
    "        for train_idx, validation_idx in sss.split(train_validation_dataset, train_validation_dataset['emotion']):\n",
    "            train_dataset = train_validation_dataset.iloc[train_idx]\n",
    "            validation_dataset = train_validation_dataset.iloc[validation_idx]\n",
    "            \n",
    "        tune_train_dataset = train_dataset\n",
    "        augmented_dataset = dataset[(dataset['augmentation'] != 'no') & (dataset['original_id'].isin(train_dataset['id']))]\n",
    "        train_dataset = pd.concat([train_dataset, augmented_dataset])\n",
    "        \n",
    "        train_features = single_block_features[0, train_dataset.index, :]\n",
    "        test_features = single_block_features[0, test_dataset.index, :]\n",
    "\n",
    "        grid_search = tune_classifier_svm(dataset, tune_train_dataset, validation_dataset, single_block_features[0])\n",
    "        classifier = grid_search.best_estimator_\n",
    "        classifier.fit(train_features, train_dataset['emotion'])\n",
    "\n",
    "        validation_accuracy = grid_search.best_score_\n",
    "        accuracies_validation.append(validation_accuracy)\n",
    "\n",
    "        predicted = classifier.predict(test_features)\n",
    "        test_accuracy = accuracy_score(predicted, test_dataset['emotion'])\n",
    "        accuracies_test.append(test_accuracy)\n",
    "        print(validation_accuracy, test_accuracy)\n",
    "    return accuracies_validation, accuracies_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-style",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_train_dataset = train_dataset[train_dataset['augmentation'] == 'no']\n",
    "grid_search = tune_classifier_svm(dataset, tune_train_dataset, validation_dataset, single_block_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_validation_single = []\n",
    "accuracies_test_single = []\n",
    "for i in range(10, 40):\n",
    "    train_dataset, validation_dataset, test_dataset = split_dataset(dataset, i, 0.7, 0.20)\n",
    "    train_features = single_block_features[0, train_dataset.index, :]\n",
    "    test_features = single_block_features[0, test_dataset.index, :]\n",
    "    \n",
    "    tune_train_dataset = train_dataset[train_dataset['augmentation'] == 'no']\n",
    "    grid_search = tune_classifier_svm(dataset, tune_train_dataset, validation_dataset, single_block_features[0])\n",
    "    classifier = grid_search.best_estimator_\n",
    "    classifier.fit(train_features, train_dataset['emotion'])\n",
    "    \n",
    "    validation_accuracy = grid_search.best_score_\n",
    "    accuracies_validation_single.append(validation_accuracy)\n",
    "    \n",
    "    predicted = classifier.predict(test_features)\n",
    "    test_accuracy = accuracy_score(predicted, test_dataset['emotion'])\n",
    "    accuracies_test_single.append(test_accuracy)\n",
    "    print(i, validation_accuracy, test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-bosnia",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_validation, accuracies_test = cross_validate(dataset, single_block_features, 10, 0.2, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-application",
   "metadata": {},
   "source": [
    "## ENSEMBLE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "practical-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble_classifier(dataset, train_dataset, validation_dataset, train_input, small_block_features, big_block_features, single_block_features):\n",
    "    grid_results = []\n",
    "    tune_train_dataset = train_dataset[train_dataset['augmentation'] == 'no']\n",
    "    tune_train_dataset = train_dataset\n",
    "\n",
    "    for i in range(0, small_block_features.shape[0]):\n",
    "        grid_results.append(tune_classifier_svm(dataset, tune_train_dataset, validation_dataset, small_block_features[i]))\n",
    "\n",
    "    for i in range(0, big_block_features.shape[0]):\n",
    "        grid_results.append(tune_classifier_svm(dataset, tune_train_dataset, validation_dataset, big_block_features[i]))\n",
    "\n",
    "    grid_results.append(tune_classifier_svm(dataset, tune_train_dataset, validation_dataset, single_block_features[0]))\n",
    "\n",
    "    weights = [results.best_score_ for results in grid_results]\n",
    "    classifiers = [('svm' + str(index), results.best_estimator_) for index, results in enumerate(grid_results)]\n",
    "\n",
    "    for index, classifier in enumerate(classifiers):\n",
    "        if classifier[1].steps[0][0] != 'transform':\n",
    "            classifier[1].steps.insert(0, ('transform', FunctionTransformer(lambda X: X[index])))\n",
    "    for classifier in classifiers:\n",
    "        classifier[1].steps[-1][1].probability = True\n",
    "\n",
    "    ensemble_classifier = VotingClassifier(estimators=classifiers, voting='soft', weights=weights, n_jobs=8)\n",
    "    ensemble_classifier.fit(train_input, train_dataset['emotion'])\n",
    "    return ensemble_classifier\n",
    "\n",
    "def train_ensemble_classifier_memory_efficient(dataset, train_dataset, validation_dataset):\n",
    "    grid_results = []\n",
    "    \n",
    "    tune_train_dataset = train_dataset[train_dataset['augmentation'] == 'no']\n",
    "    \n",
    "    n = int(1000 / len(tune_train_dataset['emotion'].unique()) / len(tune_train_dataset['sex'].unique()) / len(tune_train_dataset['dataset'].unique()))\n",
    "    tune_train_dataset = tune_train_dataset.groupby(['emotion', 'sex', 'dataset'], as_index=False, group_keys=False).apply(lambda data: data.sample(n=n, random_state=10))\n",
    "    \n",
    "    tune_train_dataset = tune_train_dataset.reset_index()\n",
    "    validation_dataset_copy = validation_dataset.copy()\n",
    "    validation_dataset_copy.index = range(len(tune_train_dataset), len(validation_dataset_copy) + len(tune_train_dataset))\n",
    "    tune_dataset = pd.concat((tune_train_dataset, validation_dataset_copy))\n",
    "    \n",
    "    small_block_features, big_block_features, single_block_features = import_dataset_features(tune_dataset)\n",
    "                                                                                              \n",
    "    for i in range(0, small_block_features.shape[0]):\n",
    "        grid_results.append(tune_classifier_svm(tune_dataset, tune_train_dataset, validation_dataset_copy, small_block_features[i]))\n",
    "\n",
    "    for i in range(0, big_block_features.shape[0]):\n",
    "        grid_results.append(tune_classifier_svm(tune_dataset, tune_train_dataset, validation_dataset_copy, big_block_features[i]))\n",
    "\n",
    "    grid_results.append(tune_classifier_svm(tune_dataset, tune_train_dataset, validation_dataset_copy, single_block_features[0]))\n",
    "\n",
    "    weights = [results.best_score_ for results in grid_results]\n",
    "    classifiers = [('svm' + str(index), results.best_estimator_) for index, results in enumerate(grid_results)]\n",
    "\n",
    "    for index, classifier in enumerate(classifiers):\n",
    "        if classifier[1].steps[0][0] != 'transform':\n",
    "            classifier[1].steps.insert(0, ('transform', FunctionTransformer(lambda X: X[index])))\n",
    "    for classifier in classifiers:\n",
    "        classifier[1].steps[-1][1].probability = True\n",
    "    \n",
    "    del small_block_features\n",
    "    del big_block_features\n",
    "    del single_block_features\n",
    "    del tune_train_dataset\n",
    "    del validation_dataset_copy\n",
    "    \n",
    "    train_input = np.concatenate(import_dataset_features(train_dataset))\n",
    "\n",
    "    ensemble_classifier = VotingClassifier(estimators=classifiers, voting='soft', weights=weights, n_jobs=8)\n",
    "    ensemble_classifier.fit(train_input, train_dataset['emotion'])\n",
    "    return ensemble_classifier\n",
    "\n",
    "def cross_validate_ensemble(dataset, small_block_features, big_block_features, single_block_features, folds, validation_dataset_size, random_state):\n",
    "    group_kfold = GroupKFold(n_splits=folds)\n",
    "    gss = GroupShuffleSplit(n_splits=1, train_size=1 - validation_dataset_size / (1 - 1.0 / folds), random_state=random_state)\n",
    "    accuracies_validation = []\n",
    "    accuracies_test = []\n",
    "    original_dataset = dataset[dataset['augmentation'] == 'no']\n",
    "    for train_validation_idx, test_idx in group_kfold.split(original_dataset, original_dataset['emotion'], original_dataset['speaker']):\n",
    "        train_validation_dataset = dataset.iloc[train_validation_idx]\n",
    "        test_dataset = dataset.iloc[test_idx]\n",
    "        for train_idx, validation_idx in gss.split(train_validation_dataset, train_validation_dataset['emotion'], train_validation_dataset['speaker']):\n",
    "            train_dataset = train_validation_dataset.iloc[train_idx]\n",
    "            validation_dataset = train_validation_dataset.iloc[validation_idx]\n",
    "        \n",
    "        augmented_dataset = dataset[(dataset['augmentation'] != 'no') & (dataset['original_id'].isin(train_dataset['id']))]\n",
    "        train_dataset = pd.concat([train_dataset, augmented_dataset])\n",
    "        \n",
    "        train_input, validation_input, test_input = split_features(train_dataset, validation_dataset, test_dataset, small_block_features, big_block_features, single_block_features)\n",
    "        ensemble_classifier = train_ensemble_classifier(dataset, train_dataset, validation_dataset, train_input, small_block_features, big_block_features, single_block_features)\n",
    "        predicted = ensemble_classifier.predict(validation_input)\n",
    "        validation_accuracy = accuracy_score(predicted, validation_dataset['emotion'])\n",
    "        accuracies_validation.append(validation_accuracy)\n",
    "        predicted = ensemble_classifier.predict(test_input)\n",
    "        test_accuracy = accuracy_score(predicted, test_dataset['emotion'])\n",
    "        accuracies_test.append(test_accuracy)\n",
    "        print(validation_accuracy, test_accuracy)\n",
    "    return accuracies_validation, accuracies_test\n",
    "\n",
    "def cross_validate_ensemble_memory_efficient(dataset, folds, validation_dataset_size, random_state):\n",
    "    group_kfold = GroupKFold(n_splits=folds)\n",
    "    gss = GroupShuffleSplit(n_splits=1, train_size=1 - validation_dataset_size / (1 - 1.0 / folds), random_state=random_state)\n",
    "    accuracies_validation = []\n",
    "    accuracies_test = []\n",
    "    predictions = []\n",
    "    fold_number = 0\n",
    "    original_dataset = dataset[dataset['augmentation'] == 'no']\n",
    "    for train_validation_idx, test_idx in group_kfold.split(original_dataset, original_dataset['emotion'], original_dataset['speaker']):\n",
    "        train_validation_dataset = dataset.iloc[train_validation_idx]\n",
    "        test_dataset = dataset.iloc[test_idx]\n",
    "        for train_idx, validation_idx in gss.split(train_validation_dataset, train_validation_dataset['emotion'], train_validation_dataset['speaker']):\n",
    "            train_dataset = train_validation_dataset.iloc[train_idx]\n",
    "            validation_dataset = train_validation_dataset.iloc[validation_idx]\n",
    "        \n",
    "        augmented_dataset = dataset[(dataset['augmentation'] != 'no') & (dataset['original_id'].isin(train_dataset['id']))]\n",
    "        train_dataset = pd.concat([train_dataset, augmented_dataset])\n",
    "               \n",
    "        ensemble_classifier = train_ensemble_classifier_memory_efficient(dataset, train_dataset, validation_dataset)\n",
    "        validation_input = np.concatenate(import_dataset_features(validation_dataset))\n",
    "        test_input = np.concatenate(import_dataset_features(test_dataset))\n",
    "        \n",
    "        predicted = ensemble_classifier.predict(validation_input)\n",
    "        validation_accuracy = accuracy_score(predicted, validation_dataset['emotion'])\n",
    "        accuracies_validation.append(validation_accuracy)\n",
    "        predicted = ensemble_classifier.predict(test_input)\n",
    "        \n",
    "        for index, prediction in enumerate(predicted):\n",
    "            row = test_dataset.iloc[index]\n",
    "            predictions.append([fold_number, row['emotion'], prediction])\n",
    "        \n",
    "        test_accuracy = accuracy_score(predicted, test_dataset['emotion'])\n",
    "        accuracies_test.append(test_accuracy)\n",
    "        fold_number += 1\n",
    "        print(validation_accuracy, test_accuracy)\n",
    "    return accuracies_validation, accuracies_test, predictions\n",
    "\n",
    "def cross_validate_ensemble_speaker_dependent(dataset, small_block_features, big_block_features, single_block_features, folds, validation_dataset_size, random_state):\n",
    "    kfold = KFold(n_splits=folds)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, train_size=1 - validation_dataset_size / (1 - 1 / folds), random_state=random_state)\n",
    "    accuracies_validation = []\n",
    "    accuracies_test = []\n",
    "    original_dataset = dataset[dataset['augmentation'] == 'no']\n",
    "    for train_validation_idx, test_idx in kfold.split(original_dataset, original_dataset['emotion']):\n",
    "        train_validation_dataset = dataset.iloc[train_validation_idx]\n",
    "        test_dataset = dataset.iloc[test_idx]\n",
    "        for train_idx, validation_idx in sss.split(train_validation_dataset, train_validation_dataset['emotion']):\n",
    "            train_dataset = train_validation_dataset.iloc[train_idx]\n",
    "            validation_dataset = train_validation_dataset.iloc[validation_idx]\n",
    "        \n",
    "        augmented_dataset = dataset[(dataset['augmentation'] != 'no') & (dataset['original_id'].isin(train_dataset['id']))]\n",
    "        train_dataset = pd.concat([train_dataset, augmented_dataset])\n",
    "        \n",
    "        train_input, validation_input, test_input = split_features(train_dataset, validation_dataset, test_dataset, small_block_features, big_block_features, single_block_features)\n",
    "        ensemble_classifier = train_ensemble_classifier(dataset, train_dataset, validation_dataset, train_input, small_block_features, big_block_features, single_block_features)\n",
    "        predicted = ensemble_classifier.predict(validation_input)\n",
    "        validation_accuracy = accuracy_score(predicted, validation_dataset['emotion'])\n",
    "        accuracies_validation.append(validation_accuracy)\n",
    "        predicted = ensemble_classifier.predict(test_input)\n",
    "        test_accuracy = accuracy_score(predicted, test_dataset['emotion'])\n",
    "        accuracies_test.append(test_accuracy)\n",
    "        print(validation_accuracy, test_accuracy)\n",
    "    return accuracies_validation, accuracies_test\n",
    "\n",
    "def cross_validate_ensemble_memory_efficient_speaker_dependent(dataset, folds, validation_dataset_size, random_state):\n",
    "    kfold = KFold(n_splits=folds)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, train_size=1 - validation_dataset_size / (1 - 1.0 / folds), random_state=random_state)\n",
    "    accuracies_validation = []\n",
    "    accuracies_test = []\n",
    "    predictions = []\n",
    "    fold_number = 0\n",
    "    original_dataset = dataset[dataset['augmentation'] == 'no']\n",
    "    for train_validation_idx, test_idx in kfold.split(original_dataset, original_dataset['emotion'], original_dataset['speaker']):\n",
    "        train_validation_dataset = dataset.iloc[train_validation_idx]\n",
    "        test_dataset = dataset.iloc[test_idx]\n",
    "        for train_idx, validation_idx in sss.split(train_validation_dataset, train_validation_dataset['emotion']):\n",
    "            train_dataset = train_validation_dataset.iloc[train_idx]\n",
    "            validation_dataset = train_validation_dataset.iloc[validation_idx]\n",
    "        \n",
    "        augmented_dataset = dataset[(dataset['augmentation'] != 'no') & (dataset['original_id'].isin(train_dataset['id']))]\n",
    "        train_dataset = pd.concat([train_dataset, augmented_dataset])\n",
    "               \n",
    "        ensemble_classifier = train_ensemble_classifier_memory_efficient(dataset, train_dataset, validation_dataset)\n",
    "        validation_input = np.concatenate(import_dataset_features(validation_dataset))\n",
    "        test_input = np.concatenate(import_dataset_features(test_dataset))\n",
    "        \n",
    "        predicted = ensemble_classifier.predict(validation_input)\n",
    "        validation_accuracy = accuracy_score(predicted, validation_dataset['emotion'])\n",
    "        accuracies_validation.append(validation_accuracy)\n",
    "        predicted = ensemble_classifier.predict(test_input)\n",
    "        \n",
    "        for index, prediction in enumerate(predicted):\n",
    "            row = test_dataset.iloc[index]\n",
    "            predictions.append([fold_number, row['emotion'], prediction])\n",
    "        \n",
    "        test_accuracy = accuracy_score(predicted, test_dataset['emotion'])\n",
    "        accuracies_test.append(test_accuracy)\n",
    "        fold_number += 1\n",
    "        print(validation_accuracy, test_accuracy)\n",
    "    return accuracies_validation, accuracies_test, predictions\n",
    "\n",
    "def leave_one_corpus_out(dataset, validation_dataset_size, random_state):\n",
    "    accuracies = []\n",
    "    corpora = dataset['dataset'].unique()\n",
    "    \n",
    "    gss = GroupShuffleSplit(n_splits=1, train_size=1 - validation_dataset_size / (1 - 1 / len(corpora)), random_state=random_state)\n",
    "    original_dataset = dataset[dataset['augmentation'] == 'no']\n",
    "    predictions = []\n",
    "    for corpus in corpora:\n",
    "        test_dataset = original_dataset[original_dataset['dataset'] == corpus]\n",
    "        train_validation_dataset = original_dataset.drop(test_dataset.index)\n",
    "        for train_idx, validation_idx in gss.split(train_validation_dataset, train_validation_dataset['emotion'], train_validation_dataset['dataset']):\n",
    "            train_dataset = train_validation_dataset.iloc[train_idx]\n",
    "            validation_dataset = train_validation_dataset.iloc[validation_idx]\n",
    "        \n",
    "        augmented_dataset = dataset[(dataset['augmentation'] != 'no') & (dataset['original_id'].isin(train_dataset['id']))]\n",
    "        train_dataset = pd.concat([train_dataset, augmented_dataset])\n",
    "        \n",
    "        ensemble_classifier = train_ensemble_classifier_memory_efficient(dataset, train_dataset, validation_dataset)\n",
    "        test_input = np.concatenate(import_dataset_features(test_dataset))\n",
    "        \n",
    "        predicted = ensemble_classifier.predict(test_input)\n",
    "        accuracy = accuracy_score(predicted, test_dataset['emotion'])\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        for index, prediction in enumerate(predicted):\n",
    "            row = test_dataset.iloc[index]\n",
    "            predictions.append([row['dataset'], row['emotion'], prediction])\n",
    "            \n",
    "        print(accuracy)\n",
    "            \n",
    "    return accuracies, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_classifier = train_ensemble_classifier(dataset, train_dataset, validation_dataset, train_input, small_block_features, big_block_features, single_block_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_validation = []\n",
    "accuracies_test = []\n",
    "for i in range(10, 40):\n",
    "    train_dataset, validation_dataset, test_dataset = split_dataset(dataset, i, 0.7, 0.2)\n",
    "    train_input, validation_input, test_input = split_features(train_dataset, validation_dataset, test_dataset, small_block_features, big_block_features, single_block_features)\n",
    "    ensemble_classifier = train_ensemble_classifier(dataset, train_dataset, validation_dataset, train_input, small_block_features, big_block_features, single_block_features)\n",
    "    \n",
    "    predicted = ensemble_classifier.predict(validation_input)\n",
    "    validation_accuracy = accuracy_score(predicted, validation_dataset['emotion'])\n",
    "    accuracies_validation.append(validation_accuracy)\n",
    "    predicted = ensemble_classifier.predict(test_input)\n",
    "    test_accuracy = accuracy_score(predicted, test_dataset['emotion'])\n",
    "    accuracies_test.append(test_accuracy)\n",
    "    print(i, validation_accuracy, test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-heaven",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_validation, accuracies_test, predictions = cross_validate_ensemble_memory_efficient_speaker_dependent(dataset, 10, 0.2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-revelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies, predictions = leave_one_corpus_out(dataset, 0.2, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
